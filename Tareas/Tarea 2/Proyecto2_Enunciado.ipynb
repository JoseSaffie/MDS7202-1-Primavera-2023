{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"b9ead8046fc74db1bf0eb99287c3c91e","deepnote_cell_type":"markdown"},"source":["![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)\n"]},{"cell_type":"markdown","metadata":{"cell_id":"ca18b36a3bd54ad09e518c5b63b08ef6","deepnote_cell_type":"markdown"},"source":["# Proyecto: Riesgo en el Banco Giturra\n","\n","**MDS7202: Laboratorio de Programación Científica para Ciencia de Datos**\n","\n","### Cuerpo Docente:\n","\n","- Profesor: Gabriel Iturra, Ignacio Meza De La Jara\n","- Auxiliar: Sebastián Tinoco\n","- Ayudante: Arturo Lazcano, Angelo Muñoz\n","\n","_Por favor, lean detalladamente las instrucciones de la tarea antes de empezar a escribir._\n","\n","---\n","\n","## Reglas\n","\n","- Fecha de entrega: 19/12/2023\n","- **Grupos de 2 personas.**\n","- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n","- Estrictamente prohibida la copia.\n","- Pueden usar cualquier material del curso que estimen conveniente.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"20bff171d2954732b74d8f532c21895a","deepnote_cell_type":"markdown"},"source":["---\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Equipo:\n","\n","- Nombre de alumno 1: Jose Ignacio Saffie\n","- Nombre de alumno 2: Matias Lopez Roman\n","\n","\n","### **Link de repositorio de GitHub:** `https://github.com/JoseSaffie/MDS7202-1-Primavera-2023.git`"]},{"cell_type":"markdown","metadata":{"cell_id":"7292d5dfddb9408d8999b7c0e63fc55c","deepnote_cell_type":"markdown"},"source":["# Presentación del Problema\n"]},{"cell_type":"markdown","metadata":{"cell_id":"4e407b3b55de4930a31e10cf4e25f51c","deepnote_cell_type":"markdown"},"source":["![](https://www.diarioeldia.cl/u/fotografias/fotosnoticias/2019/11/8/67218.jpg)\n"]},{"cell_type":"markdown","metadata":{"cell_id":"57d46ee3f60b457fa3932bf4ac15a828","deepnote_cell_type":"markdown"},"source":["**Giturra**, un banquero astuto y ambicioso, estableció su propio banco con el objetivo de obtener enormes ganancias. Sin embargo, su reputación se vio empañada debido a las tasas de interés usureras que imponía a sus clientes. A medida que su banco crecía, Giturra enfrentaba una creciente cantidad de préstamos impagados, lo que amenazaba su negocio y su prestigio.\n","\n","Para abordar este desafío, Giturra reconoció la necesidad de reducir los riesgos de préstamo y mejorar la calidad de los préstamos otorgados. Decidió aprovechar la ciencia de datos y el análisis de riesgo crediticio. Contrató a un equipo de expertos para desarrollar un modelo predictivo de riesgo crediticio.\n","\n","Cabe señalar que lo modelos solicitados por el banquero deben ser interpretables. Ya que estos le permitira al equipo comprender y explicar cómo se toman las decisiones crediticias. Utilizando visualizaciones claras y explicaciones detalladas, pudieron identificar las características más relevantes, le permitirá analizar la distribución de la importancia de las variables y evaluar si los modelos son coherentes con el negocio.\n","\n","Para esto Giturra les solicita crear un modelo de riesgo disponibilizandoles una amplia gama de variables de sus usuarios: como historiales de crédito, ingresos y otros factores financieros relevantes, para evaluar la probabilidad de incumplimiento de pago de los clientes. Con esta información, Giturra podra tomar decisiones más informadas en cuanto a los préstamos, ofreciendo condiciones más favorables a aquellos con menor riesgo de impago.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"731884b83ce840b1b52d26a180626317","deepnote_cell_type":"markdown"},"source":["## Instalación de Librerías y Carga de Datos.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"0c0b7aa1618543519231311b567d9bc3","deepnote_cell_type":"markdown"},"source":["Para el desarrollo de su proyecto, utilice el conjunto de datos `dataset.pq` para entrenar un modelo de su elección. Se le recomienda levantar un ambiente de `conda` para instalar las librerías y así evitar cualquier problema con las versiones.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"8c769111397b49e980ce325911903144","deepnote_cell_type":"markdown"},"source":["---\n","\n","## Secciones Requeridas en el Informe\n","\n","La siguiente lista detalla las secciones que debe contener su notebook para resolver el proyecto. \n","Es importante que al momento de desarrollar cada una de las secciones, estas sean escritas en un formato tipo **informe**, donde describan detalladamente cada uno de los puntos realizados.\n","\n","### 1. Introducción [0.5 puntos]\n","\n","_Esta sección es literalmente una muy breve introducción con todo lo necesario para entender que hicieron en su proyecto._\n","\n","- Describir brevemente el problema planteado (¿Qué se intenta predecir?)\n","- Describir brevemente los datos de entrada que les provee el problema.\n","- Describir las métricas que utilizarán para evaluar los modelos generados. Eligan **una métrica** adecuada para el desarrollo del proyecto **según la tarea que deben resolver y la institución a la cuál será su contraparte** y luego justifiquen su elección. Considerando que los datos presentan desbalanceo y que el uso de la métrica 'accuracy' sería incorrecto, enfoquen su elección en una de las métricas precision, recall o f1-score y en la clase que será evaluada.\n","- [Escribir al final] Describir brevemente el modelo que usaron para resolver el problema (incluyendo las transformaciones intermedias de datos).\n","- [Escribir al final] Indicar si lograron resolver el problema a través de su modelo. Indiquen además si creen que los resultados de su mejor modelo son aceptables y como les fue con respecto al resto de los equipos.\n","\n","### 2. Carga de datos Análisis Exploratorio de Datos [Sin puntaje]\n","\n","_La idea de esta sección es que cargen y exploren el dataset para así obtener una idea de como son los datos y como se relacionan con el problema._\n","\n","Cargue los datos y realice un análisis exploratorio de datos para investigar patrones, tendencias y relaciones en un conjunto de datos. Se adjuntan diversos scripts para abodar rápidamente este punto. La descripción de las columnas las pueden encontrar en el siguiente [enlace](https://www.kaggle.com/datasets/parisrohan/credit-score-classification).\n","\n","**NO deben escribir nada**, solo ejecutar el código y encontrar los patrones con los cuales se basaran para generar el modelo.\n","\n","### 3. Preparación de Datos [0.5 puntos]\n","\n","_Esta sección consiste en generar los distintos pasos para preparar sus datos con el fin de luego poder crear su modelo._\n","\n","#### 3.1 Preprocesamiento con `ColumnTransformer`\n","\n","- Convierta las columnas mal leidas a sus tipos correspondientes (float, str, etc...)\n","- Genere un `ColumnTransformer` que:\n","  - Preprocese datos categóricos y ordinales.\n","  - Escale/estandarice datos numéricos.\n","  - Uitlice `.set_output(transform=\"pandas\")` sobre su `ColumnTransformer` para setear el formato de salida a de las transformaciones a pandas.\n","\n","- Luego, pruebe las transformaciones utilizando `fit_transform`.\n","\n","- Posteriormente, ejecute un Holdout que le permita más adelante evaluar los modelos.\n","\n","#### 3.2 Holdout \n","\n","Ejecute `train_test_split` para generar un conjunto de entrenamiento, validacióny de prueba. \n","\n","#### 3.3 Datos nulos.\n","\n","Como habrá visto, existe la posibilidad de que algunos datos sean nulos. En esta sección se le solicita justificar, previo a comenzar el modelado, decidir si conservar e imputar los datos nulos o eliminar las filas. \n","\n","Note que la decisión que tomen aquí puede afectar fuertemente el rendimiento de los modelos. \n","Y como siempre, más adelante tienen el espacio para experimentar con ambas opciones.\n","\n","#### 3.4 Feature Engineering [Bonus - 0.5 puntos]\n","\n","En esta sección, se espera que apliquen su conocimiento y creatividad para identificar y construir características que brinden una mejor orientación a su modelo para identificar los casos deseados. Para motivar la construcción de nuevas características, se recomienda explorar las siguientes posibilidades:\n","\n","- Generar ratios que relacionen variables categóricas con numéricas. Estos ratios permiten capturar relaciones proporcionales o comparativas entre diferentes categorías y valores numéricos.\n","- Combinación de rankings entre variables numéricas y categóricas.\n","- Discretización de variables numéricas a categóricas.\n","- Etc...\n","\n","**Importantes**: Al explorar estas posibilidades no se limiten solo a estas propuestas, pueden aplicar otras técnicas de feature engineering pertinentes para mejorar la capacidad de su modelo para comprender y aprovechar los patrones presentes en los datos. \n","\n","### 4. Baseline [1.5 puntos]\n","\n","_En esta sección deben crear los modelos más básicos posibles que resuelvan el problema dado. La idea de estos modelos son usarlos como comparación para que en el siguiente paso lo puedan mejorar._\n","\n","Implemente, entrene y evalúe varias `Pipeline` enfocadas en resolver el problema de clasificación en donde la diferencia entre estas sea el modelo utilizado.\n","\n","\n","Para esto, cada Pipeline debe:\n","\n","- Tener el `ColumnTransformer` implementado en la sección anterior como primer paso.\n","- Implementar un imputador en caso de haber decidido conservar los datos nulos.\n","- Implementar un clasificador en la salida (ver siguiente lista).\n","  \n","Y además, \n","- Ser evaluado de forma general imprimiendo un `classification_report`.\n","- Calcular y guardar la métrica seleccionada en el punto 1.2 en un arreglo de métricas (guardar nombre y valor de la métrica).\n","\n","Lo anterior debe ser implementado utilizando los siguientes modelos:\n","\n","- `Dummy` con estrategia estratificada.\n","- `LogisticRegression`.\n","- `KNeighborsClassifier`.\n","- `DecisionTreeClassifier`\n","- `SVC`\n","- `RandomForestClassifier` \n","- `LightGBMClassifier` (del paquete `lightgbm`)\n","- `XGBClassifier` (del paquete `xgboost`).\n","\n","\n","Luego, transformando el diccionario de las métricas a un pandas `DataFrame`, ordene según los valores de su métrica de mayor a menor y responda.\n","- ¿Hay algún clasificador entrenado mejor que el azar (`Dummy`)?\n","- ¿Cuál es el mejor clasificador entrenado?\n","- ¿Por qué el mejor clasificador es mejor que los otros?\n","- Respecto al tiempo de entrenamiento, con cual cree que sería mejor experimentar (piense en el tiempo que le tomaría pasar el modelo por una grilla de optimización de hiperparámetros).\n","\n","**Nota**: Puede utilizar un for más una lista con las clases de los modelos mencionados para simplificar el proceso anterior.\n","\n","\n","### 5. Optimización del Modelo [1.5 puntos]\n","\n","_En esta sección deben mejorar del modelo de clasificación al variar los algoritmos/hiperparámetros que están ocupando._\n","\n","- Instanciar dos nuevas `Pipeline`, similares a la anterior, pero ahora enfocada en buscar el mejor modelo. Para esto, la pipelines debe utilizar el primer y segundo mejor modelo encontrado en el paso anterior.\n","- Usar **`Optuna`** para tunear hiperparámetros\n","- **Importante**: Recuerden setear la búsqueda para optimizar la métrica seleccionada en los puntos anteriores.\n","\n","Algunas ideas para mejorar el rendimiento de sus modelos:\n","\n","- Agregar técnicas de seleccion de atributos/características. El parámetro de cuántas características se seleccionan debe ser parametrizable y configurado por el optimizador de hiperparámetros.\n","- Variar el imputador de datos en caso de usarlo.\n","\n","#### Bonus\n","\n","1. **Visualización con Optuna** [0.2 extras]: Explore la documentación de visualización de Optuna en el siguiente [link](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html) y realice un análisis sobre el proceso de optimización de hiperparámetros realizado.\n","2. **Imabalanced learn** [0.3 extras]: Al ser el problema desbalanceado, pueden probar técnicas para balancear automáticamente el dataset previo a ejecutar el modelo. Para esto, puede probar con los mecanismos implementados en la librería [Imbalanced learn](https://imbalanced-learn.org/). \n","3. **Probar pycaret (AutoML)** [0.3 extras].\n","\n","Algunas notas interesantes sobre este proceso:\n","\n","- No se les pide rendimientos cercanos al 100% de la métrica para concretar exitosamente el proyecto. Por otra parte, celebren cada progreso que obtengan.\n","- **Hacer grillas computables**: Si la grilla se va a demorar 1/3 la edad del universo en explorarse completamente, entonces achíquenla a algo que sepan que va a terminar.\n","- Aprovechen el procesamiento paralelo (con `njobs`) para acelerar la búsqueda. Sin embargo, si tienen problemas con la memoria RAM, reduzca la cantidad de jobs a algo que su computador/interprete web pueda procesar.\n","\n","**Al final de este proceso, seleccione el mejor modelo encontrado, prediga el conjunto de prueba y reporte sus resultados.**"]},{"cell_type":"markdown","metadata":{"cell_id":"86fadbd406214b998dd528ec52eeecde","deepnote_cell_type":"markdown"},"source":["### 6. Interpretabilidad [1.0 puntos]\n","\n","_En esta sección, se espera que los estudiantes demuestren su capacidad para explicar cómo sus modelos toman decisiones utilizando los datos. Dentro del análisis de interpretabilidad propuesto para el modelo, deberán ser capaces de:_\n","\n","- Proponer un método para analizar la interpretabilidad del modelo. Es crucial que puedan justificar por qué el método propuesto es el más adecuado y explicar los alcances que podría tener en su aplicación.\n","- Identificar las características más relevantes del modelo. ¿La distribución de importancia es coherente y equitativa entre todas las variables?\n","- Analizar 10 observaciones aleatorias utilizando un método específico para verificar la coherencia de las interacciones entre las características.\n","- Explorar cómo se relacionan las variables utilizando algún descriptivo de interpretabilidad.\n","- ¿Existen variables irrelevantes en el problema modelado?, ¿Cuales son?.\n","\n","Es fundamental que los estudiantes sean capaces de determinar si su modelo toma decisiones coherentes y evaluar el impacto que podría tener la aplicación de un modelo con esas variables en una población. ¿Es posible que el modelo sea perjudicial o que las estimaciones se basen en decisiones sesgadas?\n","\n","En resumen, esta sección busca que los estudiantes apliquen un enfoque crítico para evaluar la interpretabilidad de su modelo, identificar posibles sesgos y analizar las implicaciones de sus decisiones en la población objetivo.\n","\n","### 7. Concluir [1.0 puntos]\n","\n","_Aquí deben escribir una breve conclusión del trabajo que hicieron en donde incluyan (pero no se limiten) a responder las siguientes preguntas:_\n","\n","- ¿Pudieron resolver exitosamente el problema?\n","- ¿Son aceptables los resultados obtenidos?\n","- ¿En que medida el EDA ayudó a comprender los datos en miras de generar un modelo predictivo?\n","\n","Respecto a la clasificación:\n","\n","- ¿Como fue el rendimiento del baseline para la clasificación?\n","- ¿Pudieron optimizar el baseline para la clasificación?\n","- ¿Que tanto mejoro el baseline de la clasificación con respecto a sus optimizaciones?\n","\n","Finalmente:\n","\n","- ¿Estuvieron conformes con sus resultados?\n","- ¿Creen que hayan mejores formas de modelar el problema?\n","- ¿En general, qué aprendieron del proyecto? ¿Qué no aprendieron y les gustaría haber aprendido?\n","\n","**OJO** si usted decide responder parte de estas preguntas, debe redactarlas en un formato de informe y no responderlas directamente.\n","\n","### Otras Instrucciones\n","\n","Recordar el uso de buenas prácticas de MLOPS como replicabilidad (fijar semillas aleatorias) o el uso del registro de experimentos (con MLFlow). Si bien son opcionales, es altamente recomendado su uso.\n","\n","### 8. Bonus: Implementación de Kedro y FastAPI [1.5 puntos]\n","\n","**OPCIONAL**\n","\n","En esta sección se les solicita utilizar las últimas tecnologías vistas en el curso para la productivización del proyecto de ciencia de datos, centrándose en la organización y gestión de los flujos de trabajo a través de componentes y pipelines, más el servicio del modelo a través del desarrollo de una API.\n","\n","Para esto: \n","\n","1. Genere un proyecto de `Kedro` en donde separe por responsabilidades los nodos/componentes de su proyecto de ciencia de datos en módulos separados. [1.0 puntos]\n","2. Genere un servidor basado en `FastAPI` el cuál a través de un método post, reciba un batch de datos y genere predicciones para cada uno de ellos. [0.5 puntos]\n","\n","Las implementaciones son libres. Es decir, usted decide qué componentes implementar, como usar el catálogo de datos y la parametrización del flujo. Sin embargo, evaluaremos buen uso de los framework, modularización y separación de responsabilidades.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["\n","### 1. Introducción [0.5 puntos]\n","\n","_Esta sección es literalmente una muy breve introducción con todo lo necesario para entender que hicieron en su proyecto._\n","\n","- Describir brevemente el problema planteado (¿Qué se intenta predecir?)\n","- Describir brevemente los datos de entrada que les provee el problema.\n","- Describir las métricas que utilizarán para evaluar los modelos generados. Eligan **una métrica** adecuada para el desarrollo del proyecto **según la tarea que deben resolver y la institución a la cuál será su contraparte** y luego justifiquen su elección. Considerando que los datos presentan desbalanceo y que el uso de la métrica 'accuracy' sería incorrecto, enfoquen su elección en una de las métricas precision, recall o f1-score y en la clase que será evaluada.\n","- [Escribir al final] Describir brevemente el modelo que usaron para resolver el problema (incluyendo las transformaciones intermedias de datos).\n","- [Escribir al final] Indicar si lograron resolver el problema a través de su modelo. Indiquen además si creen que los resultados de su mejor modelo son aceptables y como les fue con respecto al resto de los equipos."]},{"cell_type":"code","execution_count":177,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["credit_score\n","0    63143.076702\n","1    48828.348776\n","Name: annual_income, dtype: float64\n"]}],"source":["resultado = data_preprocessed.groupby('credit_score')['annual_income'].mean()\n","         \n","# Mostrar el resultado\n","print(resultado)"]},{"cell_type":"markdown","metadata":{},"source":["- Describir brevemente el problema planteado (¿Qué se intenta predecir?)\n","\n","Los problemas presentados en el banco consisten de el riesgo presente en los prestamos y la calidad de estos. La idea es que sean resultados que se puedan interpretar ya que el equipo tiene que tener la capacidad de comprender y explicar porque se toman las decisiones crediticias, ya que todo lo relacionado con modelos de riesgos implican temas éticos y morales. Se intenta predecir si un cliente va a ser riesgos o no, si credit score es 1, es porque el cliente es riegosos\n","\n","\n","- Describir brevemente los datos de entrada que les provee el problema.\n","\n","En resumen\n","Hay información de del historial crediticio, distinos niveles de agregacion de ingresos, facotres financieros relevantes, incumplimiento de pago, entre otro.\n","\n","\n","En detalle:\n","\n","Customer_id: Identificación del cliente. Se eliminará ya que todos son diferentes.\n","\n","age: Edad del cliente. \n","\n","ocuppation: Trabajo/ocupación del cliente.\n","\n","annual_income: Ingreso anual del cliente\n","\n","monthly_inhand_salary: Salario mensual neto en mano del cliente.\n","\n","num_bank_accounts: Número de cuentas bancarias.\n","\n","num_credit_card: Número de tarjetas de crédito.\n","\n","interest_rate: Tasa de interés.\n","\n","num_of_loan: Número de préstamos. \n","\n","delay_from_due_date: Retraso desde la fecha de vencimiento. \n","\n","num_of_delayed_payment: Número de pagos retrasados. \n","\n","changed_credit_limit: Cambio en el límite de crédito. \n","\n","num_credit_inquiries: Número de consultas de crédito.\n","\n","outstanding_debt: Deuda pendiente. No requiere cambios.\n","\n","credit_utilization_ratio: Tasa de utilización de crédito. \n","\n","credit_history_age: Antigüedad del historial crediticio.\n","\n","payment_of_min_amount: Pago del monto mínimo. \n","\n","total_emi_per_month: Cuota total mensual.\n","\n","amount_invested_monthly: Monto invertido mensualmente. \n","\n","payment_behaviour: Comportamiento de pago. \n","\n","monthly_balance: Saldo mensual.\n","\n","credit_score: Puntuación crediticia. Debe ser un número entre 1 y 0.\n","\n","\n","- Describir las métricas que utilizarán para evaluar los modelos generados. Eligan **una métrica** adecuada para el desarrollo del proyecto **según la tarea que deben resolver y la institución a la cuál será su contraparte** y luego justifiquen su elección. Considerando que los datos presentan desbalanceo y que el uso de la métrica 'accuracy' sería incorrecto, enfoquen su elección en una de las métricas precision, recall o f1-score y en la clase que será evaluada.\n","\n","\n","Lo más importante del modelo va a ser reconocer cuales son los clientes riesgosos, ya que estos generaran perdidas y hay que estar atentos a ellos, por lo tanto vamos a escoger la métrica de recall como más adecuada ya que nos permitira seleccionar el modelo con menos casos de casos positivos ( Sean los casos positivos credi_Score = 1). Igualmente nos interesa saber el f1-score para saber el desempeño general del modelo\n"]},{"cell_type":"markdown","metadata":{},"source":["- [Escribir al final] Describir brevemente el modelo que usaron para resolver el problema (incluyendo las transformaciones intermedias de datos).\n","- [Escribir al final] Indicar si lograron resolver el problema a través de su modelo. Indiquen además si creen que los resultados de su mejor modelo son aceptables y como les fue con respecto al resto de los equipos. "]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=87110296-876e-426f-b91d-aaf681223468' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Preparación de Datos [0.5 puntos]\n","\n","_Esta sección consiste en generar los distintos pasos para preparar sus datos con el fin de luego poder crear su modelo._\n","\n","#### 3.1 Preprocesamiento con `ColumnTransformer`\n","\n","- Convierta las columnas mal leidas a sus tipos correspondientes (float, str, etc...)\n","- Genere un `ColumnTransformer` que:\n","  - Preprocese datos categóricos y ordinales.\n","  - Escale/estandarice datos numéricos.\n","  - Uitlice `.set_output(transform=\"pandas\")` sobre su `ColumnTransformer` para setear el formato de salida a de las transformaciones a pandas.\n","\n","- Luego, pruebe las transformaciones utilizando `fit_transform`.\n","\n","- Posteriormente, ejecute un Holdout que le permita más adelante evaluar los modelos.\n","\n","#### 3.2 Holdout \n","\n","Ejecute `train_test_split` para generar un conjunto de entrenamiento, validacióny de prueba. \n","\n","#### 3.3 Datos nulos.\n","\n","Como habrá visto, existe la posibilidad de que algunos datos sean nulos. En esta sección se le solicita justificar, previo a comenzar el modelado, decidir si conservar e imputar los datos nulos o eliminar las filas. \n","\n","Note que la decisión que tomen aquí puede afectar fuertemente el rendimiento de los modelos. \n","Y como siempre, más adelante tienen el espacio para experimentar con ambas opciones.\n","\n","#### 3.4 Feature Engineering [Bonus - 0.5 puntos]\n","\n","En esta sección, se espera que apliquen su conocimiento y creatividad para identificar y construir características que brinden una mejor orientación a su modelo para identificar los casos deseados. Para motivar la construcción de nuevas características, se recomienda explorar las siguientes posibilidades:\n","\n","- Generar ratios que relacionen variables categóricas con numéricas. Estos ratios permiten capturar relaciones proporcionales o comparativas entre diferentes categorías y valores numéricos.\n","- Combinación de rankings entre variables numéricas y categóricas.\n","- Discretización de variables numéricas a categóricas.\n","- Etc...\n","\n","**Importantes**: Al explorar estas posibilidades no se limiten solo a estas propuestas, pueden aplicar otras técnicas de feature engineering pertinentes para mejorar la capacidad de su modelo para comprender y aprovechar los patrones presentes en los datos. "]},{"cell_type":"code","execution_count":178,"metadata":{},"outputs":[],"source":["# Librerias\n","\n","import pandas as pd"]},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[{"data":{"text/plain":["customer_id                  object\n","age                         float64\n","occupation                   object\n","annual_income               float64\n","monthly_inhand_salary       float64\n","num_bank_accounts             int64\n","num_credit_card               int64\n","interest_rate                 int64\n","num_of_loan                 float64\n","delay_from_due_date           int64\n","num_of_delayed_payment      float64\n","changed_credit_limit        float64\n","num_credit_inquiries        float64\n","outstanding_debt            float64\n","credit_utilization_ratio    float64\n","credit_history_age          float64\n","payment_of_min_amount        object\n","total_emi_per_month         float64\n","amount_invested_monthly     float64\n","payment_behaviour            object\n","monthly_balance             float64\n","credit_score                  int64\n","dtype: object"]},"execution_count":179,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_parquet('dataset.pq')\n","df.dtypes"]},{"cell_type":"code","execution_count":180,"metadata":{},"outputs":[],"source":["df = pd.read_parquet('dataset.pq')"]},{"cell_type":"code","execution_count":181,"metadata":{},"outputs":[],"source":["# Preprocesamiento de los datos, incluye el tema de los datos nulos\n","# Analizando cada columna note las siguiente acciones que hay realizar por columna\n","\n","# Customer_id                , dejarlo igual, da lo mismo, se va a borrar ( todos son diferentes)\n","# age                        , solo permitir datos entre 16 y 120\n","# ocuppation                 , hay que pasarlo a dummies\n","# annual_income              , tiene que ser un numero entre 0 Y 1.000.000    95TH I.C = 134.316\n","# monthly_inhand_salary      , tenemos que imputar datos con knn\n","# num_bank_accounts          , ok tiene que ser un numero entre 0 y 40  95TH I.C = 10\n","# num_credit_card            , ok tiene que ser un numero entre 0 y 40  95TH I.C = 10\n","# interest_rate              , entre 0 y 50, ya que 95th I.C es 33\n","# num_of_loan                , entre 0 y 30, ya que 95th I.C es 8\n","# delay_from_due_date        , ok los negativos podrian tener sentido\n","# num_of_delayed_payment     , ok los negativos podrian tener sentido, pero entre -3 y 100  95-th IC = 24, si es missing value que lo pase a 0 ( es un supuesto)\n","# changed_credit_limit       , ok los negativos podrian tener sentido, pero entre -7 y 100  95-th IC = 24\n","# num_credit_inquiries       , entre 0 y 60 95-th IC =14\n","# outstanding_debt           , esta ok\n","#credit_utilization_ratio    , esta ok\n","# credit_history_age         , Los NaN pasarlos a 0\n","#payment_of_min_amount       , pasarle los nm a no, luego pasarlo a dummie\n","#total_emi_per_month         ,  95-th percentile\t592.3190739, los datos entre 0 y 1000\n","#amount_invested_monthly     , entre 0 y 2000, 95-th percentile\t1154.91547, knn para imputar a los missing  \n","#payment_behaviour           , dummies de todos, vamos a mantener  9#%8, tal vez describe como que no hay ningun comportamiento visible\n","#monthly_balance             , entre 0 y 1500, los missing que ponga por knn\n","#credit_score                , numero entre 1 y 0\n","\n","# df = pd.DataFrame(data)\n","import pandas as pd\n","from sklearn.impute import KNNImputer\n","from sklearn.preprocessing import StandardScaler\n","\n","\n","\n","# 3.3\n","\n","def preprocess_data(df):\n","    \n","    # Identificar columnas numéricas y categóricas\n","    numeric_features = df.select_dtypes(include=['int64', 'float64']).columns\n","    categorical_features = df.select_dtypes(include=['object']).columns \n","    # 1. Filtrar 'age' entre 16 y 120\n","    df['age'] = df['age'].apply(lambda x: max(16, min(120, x)))\n","\n","    # 2. Convertir 'ocuppation' a dummies\n","    #df = pd.get_dummies(df, columns=['ocuppation'], prefix='ocuppation')\n","\n","    # 3. Limitar 'annual_income' entre 0 y 1,000,000\n","    df['annual_income'] = df['annual_income'].apply(lambda x: max(0, min(1000000, x)))\n","\n","    # 4. Imputar 'monthly_inhand_salary' con KNN\n","    imputer = KNNImputer(n_neighbors=5)\n","    #df['monthly_inhand_salary'] = imputer.fit_transform(df[['monthly_inhand_salary']])\n","\n","    # 5. Limitar 'num_bank_accounts' y 'num_credit_card' entre 0 y 40\n","    df['num_bank_accounts'] = df['num_bank_accounts'].apply(lambda x: max(0, min(40, x)))\n","    df['num_credit_card'] = df['num_credit_card'].apply(lambda x: max(0, min(40, x)))\n","\n","    # 6. Limitar 'interest_rate' entre 0 y 50\n","    df['interest_rate'] = df['interest_rate'].apply(lambda x: max(0, min(50, x)))\n","\n","    # 7. Limitar 'num_of_loan' entre 0 y 30\n","    df['num_of_loan'] = df['num_of_loan'].apply(lambda x: max(0, min(30, x)))\n","\n","    # 8. Limitar 'num_of_delayed_payment' entre -3 y 100, y rellenar NaN con 0\n","    df['num_of_delayed_payment'] = df['num_of_delayed_payment'].apply(lambda x: max(-3, min(100, x)))\n","    df['num_of_delayed_payment'].fillna(0, inplace=True)\n","\n","    # 9. Limitar 'changed_credit_limit' entre -7 y 100\n","    df['changed_credit_limit'] = df['changed_credit_limit'].apply(lambda x: max(-7, min(100, x)))\n","\n","    # 10. Limitar 'num_credit_inquiries' entre 0 y 60\n","    df['num_credit_inquiries'] = df['num_credit_inquiries'].apply(lambda x: max(0, min(60, x)))\n","\n","    # 11. Imputar 'amount_invested_monthly' con KNN\n","    #df['amount_invested_monthly'] = imputer.fit_transform(df[['amount_invested_monthly']])\n","\n","    # 12. Limitar 'total_emi_per_month' entre 0 y 1000\n","    df['total_emi_per_month'] = df['total_emi_per_month'].apply(lambda x: max(0, min(1000, x)))\n","\n","    # 13. Limitar 'monthly_balance' entre 0 y 1500, y rellenar missing con KNN\n","    df['monthly_balance'] = df['monthly_balance'].apply(lambda x: max(0, min(1500, x)))\n","    #df['monthly_balance'] = imputer.fit_transform(df[['monthly_balance']])\n","\n","    # 14. Rellenar NaN en 'credit_history_age' con 0\n","    df['credit_history_age'].fillna(0, inplace=True)\n","\n","    # 15. Convertir 'payment_of_min_amount' a dummies\n","    #df = pd.get_dummies(df, columns=['payment_of_min_amount'], prefix='payment_of_min_amount')\n","    df['payment_of_min_amount'] = df['payment_of_min_amount'].apply(lambda x: 'no' if x == 'nm' else x)\n","\n","    # 16. Limitar 'credit_score' a int64\n","    df['credit_score'] = df['credit_score'].astype('int64')\n","\n","    df = df.drop('customer_id', axis=1)\n","\n","    return df\n","\n","# Uso de la función\n","# Supongamos que tienes un DataFrame llamado 'data'\n","data_preprocessed = preprocess_data(df.copy())\n"]},{"cell_type":"code","execution_count":182,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>occupation_Accountant</th>\n","      <th>occupation_Architect</th>\n","      <th>occupation_Developer</th>\n","      <th>occupation_Doctor</th>\n","      <th>occupation_Engineer</th>\n","      <th>occupation_Entrepreneur</th>\n","      <th>occupation_Journalist</th>\n","      <th>occupation_Lawyer</th>\n","      <th>occupation_Manager</th>\n","      <th>occupation_Mechanic</th>\n","      <th>...</th>\n","      <th>num_of_delayed_payment</th>\n","      <th>changed_credit_limit</th>\n","      <th>num_credit_inquiries</th>\n","      <th>outstanding_debt</th>\n","      <th>credit_utilization_ratio</th>\n","      <th>credit_history_age</th>\n","      <th>total_emi_per_month</th>\n","      <th>amount_invested_monthly</th>\n","      <th>monthly_balance</th>\n","      <th>credit_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>-0.585424</td>\n","      <td>-0.066283</td>\n","      <td>-0.427045</td>\n","      <td>-0.533485</td>\n","      <td>-1.631977</td>\n","      <td>-1.750898</td>\n","      <td>-0.452667</td>\n","      <td>-0.299649</td>\n","      <td>-0.273692</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>-0.670161</td>\n","      <td>-0.474756</td>\n","      <td>-0.615446</td>\n","      <td>-0.710911</td>\n","      <td>0.113367</td>\n","      <td>1.097470</td>\n","      <td>-0.599304</td>\n","      <td>-0.204916</td>\n","      <td>-0.281060</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>-0.585424</td>\n","      <td>-0.357451</td>\n","      <td>-0.521246</td>\n","      <td>-0.106664</td>\n","      <td>1.168498</td>\n","      <td>0.148014</td>\n","      <td>0.488492</td>\n","      <td>4.568425</td>\n","      <td>1.661532</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>-0.458319</td>\n","      <td>-0.714254</td>\n","      <td>-0.427045</td>\n","      <td>-0.687165</td>\n","      <td>-0.972878</td>\n","      <td>0.042519</td>\n","      <td>-0.610750</td>\n","      <td>-0.250441</td>\n","      <td>-0.197733</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>-0.204110</td>\n","      <td>-0.673058</td>\n","      <td>-0.427045</td>\n","      <td>-0.417583</td>\n","      <td>-1.257870</td>\n","      <td>1.519450</td>\n","      <td>-0.689008</td>\n","      <td>-0.223252</td>\n","      <td>-0.252531</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>12495</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>3.397191</td>\n","      <td>-0.497100</td>\n","      <td>-0.709647</td>\n","      <td>-0.300176</td>\n","      <td>0.653017</td>\n","      <td>1.202965</td>\n","      <td>-0.522268</td>\n","      <td>-0.255532</td>\n","      <td>3.838529</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>12496</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>-0.839634</td>\n","      <td>-0.455904</td>\n","      <td>-0.050243</td>\n","      <td>-0.458012</td>\n","      <td>1.718785</td>\n","      <td>-1.750898</td>\n","      <td>-0.493005</td>\n","      <td>-0.277190</td>\n","      <td>3.838529</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>12497</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>-0.458319</td>\n","      <td>-0.477549</td>\n","      <td>-0.521246</td>\n","      <td>-0.697398</td>\n","      <td>1.348118</td>\n","      <td>1.413955</td>\n","      <td>-0.287568</td>\n","      <td>-0.290791</td>\n","      <td>3.838529</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>12498</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.219573</td>\n","      <td>0.425281</td>\n","      <td>0.043958</td>\n","      <td>1.857360</td>\n","      <td>0.929200</td>\n","      <td>-1.117928</td>\n","      <td>-0.398367</td>\n","      <td>-0.294829</td>\n","      <td>3.838529</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>12499</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>-0.585424</td>\n","      <td>-0.050224</td>\n","      <td>-0.521246</td>\n","      <td>-0.799776</td>\n","      <td>0.357444</td>\n","      <td>1.519450</td>\n","      <td>-0.521655</td>\n","      <td>-0.230166</td>\n","      <td>3.838529</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>12500 rows × 44 columns</p>\n","</div>"],"text/plain":["       occupation_Accountant  occupation_Architect  occupation_Developer  \\\n","0                        0.0                   0.0                   0.0   \n","1                        0.0                   0.0                   0.0   \n","2                        0.0                   0.0                   0.0   \n","3                        0.0                   0.0                   0.0   \n","4                        0.0                   0.0                   1.0   \n","...                      ...                   ...                   ...   \n","12495                    0.0                   0.0                   0.0   \n","12496                    0.0                   0.0                   0.0   \n","12497                    0.0                   0.0                   0.0   \n","12498                    0.0                   1.0                   0.0   \n","12499                    0.0                   0.0                   0.0   \n","\n","       occupation_Doctor  occupation_Engineer  occupation_Entrepreneur  \\\n","0                    0.0                  0.0                      0.0   \n","1                    0.0                  0.0                      0.0   \n","2                    0.0                  1.0                      0.0   \n","3                    0.0                  0.0                      1.0   \n","4                    0.0                  0.0                      0.0   \n","...                  ...                  ...                      ...   \n","12495                0.0                  0.0                      0.0   \n","12496                0.0                  0.0                      0.0   \n","12497                0.0                  0.0                      0.0   \n","12498                0.0                  0.0                      0.0   \n","12499                0.0                  0.0                      0.0   \n","\n","       occupation_Journalist  occupation_Lawyer  occupation_Manager  \\\n","0                        0.0                0.0                 0.0   \n","1                        0.0                0.0                 0.0   \n","2                        0.0                0.0                 0.0   \n","3                        0.0                0.0                 0.0   \n","4                        0.0                0.0                 0.0   \n","...                      ...                ...                 ...   \n","12495                    0.0                1.0                 0.0   \n","12496                    0.0                0.0                 0.0   \n","12497                    0.0                0.0                 0.0   \n","12498                    0.0                0.0                 0.0   \n","12499                    0.0                0.0                 0.0   \n","\n","       occupation_Mechanic  ...  num_of_delayed_payment  changed_credit_limit  \\\n","0                      0.0  ...               -0.585424             -0.066283   \n","1                      0.0  ...               -0.670161             -0.474756   \n","2                      0.0  ...               -0.585424             -0.357451   \n","3                      0.0  ...               -0.458319             -0.714254   \n","4                      0.0  ...               -0.204110             -0.673058   \n","...                    ...  ...                     ...                   ...   \n","12495                  0.0  ...                3.397191             -0.497100   \n","12496                  0.0  ...               -0.839634             -0.455904   \n","12497                  0.0  ...               -0.458319             -0.477549   \n","12498                  0.0  ...                0.219573              0.425281   \n","12499                  1.0  ...               -0.585424             -0.050224   \n","\n","       num_credit_inquiries  outstanding_debt  credit_utilization_ratio  \\\n","0                 -0.427045         -0.533485                 -1.631977   \n","1                 -0.615446         -0.710911                  0.113367   \n","2                 -0.521246         -0.106664                  1.168498   \n","3                 -0.427045         -0.687165                 -0.972878   \n","4                 -0.427045         -0.417583                 -1.257870   \n","...                     ...               ...                       ...   \n","12495             -0.709647         -0.300176                  0.653017   \n","12496             -0.050243         -0.458012                  1.718785   \n","12497             -0.521246         -0.697398                  1.348118   \n","12498              0.043958          1.857360                  0.929200   \n","12499             -0.521246         -0.799776                  0.357444   \n","\n","       credit_history_age  total_emi_per_month  amount_invested_monthly  \\\n","0               -1.750898            -0.452667                -0.299649   \n","1                1.097470            -0.599304                -0.204916   \n","2                0.148014             0.488492                 4.568425   \n","3                0.042519            -0.610750                -0.250441   \n","4                1.519450            -0.689008                -0.223252   \n","...                   ...                  ...                      ...   \n","12495            1.202965            -0.522268                -0.255532   \n","12496           -1.750898            -0.493005                -0.277190   \n","12497            1.413955            -0.287568                -0.290791   \n","12498           -1.117928            -0.398367                -0.294829   \n","12499            1.519450            -0.521655                -0.230166   \n","\n","       monthly_balance  credit_score  \n","0            -0.273692           0.0  \n","1            -0.281060           0.0  \n","2             1.661532           0.0  \n","3            -0.197733           0.0  \n","4            -0.252531           0.0  \n","...                ...           ...  \n","12495         3.838529           0.0  \n","12496         3.838529           0.0  \n","12497         3.838529           0.0  \n","12498         3.838529           0.0  \n","12499         3.838529           1.0  \n","\n","[12500 rows x 44 columns]"]},"execution_count":182,"metadata":{},"output_type":"execute_result"}],"source":["#3.1\n","\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","import pandas as pd\n","\n","# Definir las columnas\n","categorical_cols = ['occupation', 'payment_of_min_amount', 'payment_behaviour']\n","numeric_cols = ['age' ,'annual_income', 'monthly_inhand_salary', 'num_bank_accounts', 'num_credit_card', 'interest_rate',\n","                'num_of_loan', 'delay_from_due_date', 'num_of_delayed_payment', 'changed_credit_limit',\n","                'num_credit_inquiries', 'outstanding_debt', 'credit_utilization_ratio', 'credit_history_age',\n","                'total_emi_per_month', 'amount_invested_monthly', 'monthly_balance']\n","ID_cols= ['customer_id']\n","#to_pred=['credit_score']\n","\n","# Crear transformers para las columnas categóricas y numéricas\n","categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n","numeric_transformer = StandardScaler()\n","\n","# Crear preprocessor usando ColumnTransformer\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('cat', categorical_transformer, categorical_cols),\n","        ('num', numeric_transformer, numeric_cols),\n","       # ('y',   'passthrough'  ,to_pred),\n","        \n","    ],\n","    remainder='passthrough'\n",")\n","\n","# Crear el pipeline con el preprocessor\n","pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n","\n","# Aplicar el pipeline al conjunto de datos\n","X_transformed = pipeline.fit_transform(data_preprocessed)\n","\n","# Obtener los nombres de las columnas después de la transformación\n","feature_names = (list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)) +\n","                 numeric_cols + to_pred )\n","\n","# Crear un DataFrame con los datos transformados y los nombres de las columnas\n","X_transformed_df = pd.DataFrame(X_transformed, columns=feature_names)\n","\n","# Mostrar el conjunto de datos transformado\n","X_transformed_df\n"]},{"cell_type":"code","execution_count":183,"metadata":{},"outputs":[],"source":["#3.2\n","from sklearn.model_selection import train_test_split\n","\n","X = X_transformed_df.drop('credit_score', axis=1)\n","y = X_transformed_df['credit_score']\n","\n","# Suponiendo que 'credit_score' es tu variable objetivo\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","# Puedes usar X_train, y_train para entrenar, X_val, y_val para validar, y X_test, y_test para probar tus modelos\n"]},{"cell_type":"code","execution_count":184,"metadata":{},"outputs":[{"data":{"text/plain":["0        0.0\n","1        0.0\n","2        0.0\n","3        0.0\n","4        0.0\n","        ... \n","12495    0.0\n","12496    0.0\n","12497    0.0\n","12498    0.0\n","12499    1.0\n","Name: credit_score, Length: 12500, dtype: float64"]},"execution_count":184,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","### 4. Baseline [1.5 puntos]\n","\n","\n","_En esta sección deben crear los modelos más básicos posibles que resuelvan el problema dado. La idea de estos modelos son usarlos como comparación para que en el siguiente paso lo puedan mejorar._\n","\n","Implemente, entrene y evalúe varias `Pipeline` enfocadas en resolver el problema de clasificación en donde la diferencia entre estas sea el modelo utilizado.\n","\n","\n","Para esto, cada Pipeline debe:\n","\n","- Tener el `ColumnTransformer` implementado en la sección anterior como primer paso.\n","- Implementar un imputador en caso de haber decidido conservar los datos nulos.\n","- Implementar un clasificador en la salida (ver siguiente lista).\n","  \n","Y además, \n","- Ser evaluado de forma general imprimiendo un `classification_report`.\n","- Calcular y guardar la métrica seleccionada en el punto 1.2 en un arreglo de métricas (guardar nombre y valor de la métrica).\n","\n","Lo anterior debe ser implementado utilizando los siguientes modelos:\n","\n","- `Dummy` con estrategia estratificada.\n","- `LogisticRegression`.\n","- `KNeighborsClassifier`.\n","- `DecisionTreeClassifier`\n","- `SVC`\n","- `RandomForestClassifier` \n","- `LightGBMClassifier` (del paquete `lightgbm`)\n","- `XGBClassifier` (del paquete `xgboost`).\n","\n","**Nota**: Puede utilizar un for más una lista con las clases de los modelos mencionados para simplificar el proceso anterior.\n","\n"]},{"cell_type":"code","execution_count":185,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.dummy import DummyClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from lightgbm import LGBMClassifier\n","from xgboost import XGBClassifier\n","from sklearn.metrics import recall_score, f1_score\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n"]},{"cell_type":"code","execution_count":186,"metadata":{},"outputs":[],"source":["# Columntransformer esta implementado\n","# La imputación la realizo antes, con knn en la funcion\n","\n","\n","# Explicado más arriba\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('cat', categorical_transformer, categorical_cols),\n","        ('num', numeric_transformer, numeric_cols)\n","   #     ('y',   'passthrough'  ,to_pred),\n","        \n","    ],\n","    remainder='passthrough'\n",")\n","\n","X = X_transformed_df.drop('credit_score', axis=1)\n","y = X_transformed_df['credit_score']\n","\n","\n","X = data_preprocessed.drop('credit_score', axis=1)\n","y = data_preprocessed['credit_score'].ravel()\n","\n","#data_preprocessed\n","#data_preprocessed\n","\n","# Suponiendo que 'credit_score' es tu variable objetivo\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"]},{"cell_type":"code","execution_count":187,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification Report for Dummy:\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.71      0.71      1318\n","           1       0.30      0.29      0.29       557\n","\n","    accuracy                           0.59      1875\n","   macro avg       0.50      0.50      0.50      1875\n","weighted avg       0.58      0.59      0.58      1875\n","\n","[[936 382]\n"," [395 162]]\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\josei\\AppData\\Local\\Temp\\ipykernel_16204\\877093562.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  metrics_df = pd.concat([metrics_df, pd.DataFrame({'Classifier': [name], 'Recall': [recall], 'F1-Score': [f1]})], ignore_index=True)\n"]},{"name":"stdout","output_type":"stream","text":["Classification Report for LogisticRegression:\n","              precision    recall  f1-score   support\n","\n","           0       0.76      0.89      0.82      1318\n","           1       0.57      0.35      0.43       557\n","\n","    accuracy                           0.73      1875\n","   macro avg       0.67      0.62      0.63      1875\n","weighted avg       0.71      0.73      0.71      1875\n","\n","[[1169  149]\n"," [ 362  195]]\n","Classification Report for KNeighbors:\n","              precision    recall  f1-score   support\n","\n","           0       0.79      0.85      0.82      1318\n","           1       0.57      0.46      0.51       557\n","\n","    accuracy                           0.74      1875\n","   macro avg       0.68      0.66      0.66      1875\n","weighted avg       0.72      0.74      0.73      1875\n","\n","[[1124  194]\n"," [ 301  256]]\n","Classification Report for DecisionTree:\n","              precision    recall  f1-score   support\n","\n","           0       0.78      0.77      0.77      1318\n","           1       0.47      0.48      0.47       557\n","\n","    accuracy                           0.68      1875\n","   macro avg       0.62      0.62      0.62      1875\n","weighted avg       0.68      0.68      0.68      1875\n","\n","[[1016  302]\n"," [ 292  265]]\n","Classification Report for SVM:\n","              precision    recall  f1-score   support\n","\n","           0       0.80      0.88      0.84      1318\n","           1       0.63      0.48      0.55       557\n","\n","    accuracy                           0.76      1875\n","   macro avg       0.72      0.68      0.69      1875\n","weighted avg       0.75      0.76      0.75      1875\n","\n","[[1162  156]\n"," [ 287  270]]\n","Classification Report for RandomForest:\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.88      0.85      1318\n","           1       0.65      0.53      0.59       557\n","\n","    accuracy                           0.78      1875\n","   macro avg       0.73      0.71      0.72      1875\n","weighted avg       0.77      0.78      0.77      1875\n","\n","[[1161  157]\n"," [ 261  296]]\n","[LightGBM] [Info] Number of positive: 2537, number of negative: 6213\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000704 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 2372\n","[LightGBM] [Info] Number of data points in the train set: 8750, number of used features: 43\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.289943 -> initscore=-0.895662\n","[LightGBM] [Info] Start training from score -0.895662\n","Classification Report for LightGBM:\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.87      0.85      1318\n","           1       0.65      0.56      0.60       557\n","\n","    accuracy                           0.78      1875\n","   macro avg       0.74      0.72      0.72      1875\n","weighted avg       0.77      0.78      0.78      1875\n","\n","[[1152  166]\n"," [ 246  311]]\n","Classification Report for XGBoost:\n","              precision    recall  f1-score   support\n","\n","           0       0.81      0.87      0.84      1318\n","           1       0.63      0.53      0.57       557\n","\n","    accuracy                           0.77      1875\n","   macro avg       0.72      0.70      0.71      1875\n","weighted avg       0.76      0.77      0.76      1875\n","\n","[[1145  173]\n"," [ 264  293]]\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Classifier</th>\n","      <th>Recall</th>\n","      <th>F1-Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>6</th>\n","      <td>LightGBM</td>\n","      <td>0.558348</td>\n","      <td>0.601547</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>RandomForest</td>\n","      <td>0.531418</td>\n","      <td>0.586139</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>XGBoost</td>\n","      <td>0.526032</td>\n","      <td>0.572825</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>SVM</td>\n","      <td>0.484740</td>\n","      <td>0.549339</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>DecisionTree</td>\n","      <td>0.475763</td>\n","      <td>0.471530</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>KNeighbors</td>\n","      <td>0.459605</td>\n","      <td>0.508441</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LogisticRegression</td>\n","      <td>0.350090</td>\n","      <td>0.432852</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Dummy</td>\n","      <td>0.290844</td>\n","      <td>0.294278</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Classifier    Recall  F1-Score\n","6            LightGBM  0.558348  0.601547\n","5        RandomForest  0.531418  0.586139\n","7             XGBoost  0.526032  0.572825\n","4                 SVM  0.484740  0.549339\n","3        DecisionTree  0.475763  0.471530\n","2          KNeighbors  0.459605  0.508441\n","1  LogisticRegression  0.350090  0.432852\n","0               Dummy  0.290844  0.294278"]},"execution_count":187,"metadata":{},"output_type":"execute_result"}],"source":["# Imputador KNN para manejar valores nulos\n","knn_imputer = KNNImputer(n_neighbors=5)  # Puedes ajustar el número de vecinos según tus necesidades\n","from sklearn.metrics import confusion_matrix\n","\n","classifiers = [\n","    ('Dummy', DummyClassifier(strategy='stratified')),\n","    ('LogisticRegression', LogisticRegression()),\n","    ('KNeighbors', KNeighborsClassifier()),\n","    ('DecisionTree', DecisionTreeClassifier()),\n","    ('SVM', SVC()),\n","    ('RandomForest', RandomForestClassifier()),\n","    ('LightGBM', LGBMClassifier()),\n","    ('XGBoost', XGBClassifier())\n","]\n","\n","# Crear un DataFrame para almacenar las métricas\n","metrics_df = pd.DataFrame(columns=['Classifier', 'Recall', 'F1-Score'])\n","\n","for name, classifier in classifiers:\n","    # Construir el pipeline\n","    pipeline = Pipeline([\n","        ('preprocessor', preprocessor),\n","        ('knn_imputer', knn_imputer),  # Agrega el imputador KNN al pipeline\n","        ('classifier', classifier)\n","    ])\n","\n","    # Entrenar el modelo\n","    pipeline.fit(X_train, y_train)\n","\n","    # Realizar predicciones\n","    y_pred = pipeline.predict(X_test)\n","\n","    # Imprimir classification report\n","    print(\" ------------------------------- \")\n","    print(f\"Classification Report for {name}:\")\n","    print(classification_report(y_test, y_pred))\n","    print(confusion_matrix(y_test, y_pred))\n","    print(\" ------------------------------- \")\n","    \n","    # Calcular y guardar métricas\n","    recall = recall_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","       # Usar concat en lugar de append\n","    metrics_df = pd.concat([metrics_df, pd.DataFrame({'Classifier': [name], 'Recall': [recall], 'F1-Score': [f1]})], ignore_index=True)\n","\n","# Ordenar el DataFrame por Recall de mayor a menor\n","metrics_df = metrics_df.sort_values(by='Recall', ascending=False)\n","metrics_df"]},{"cell_type":"markdown","metadata":{},"source":["\n","Luego, transformando el diccionario de las métricas a un pandas `DataFrame`, ordene según los valores de su métrica de mayor a menor y responda.\n","- ¿Hay algún clasificador entrenado mejor que el azar (`Dummy`)?\n","\n","Todos los modelos logran un mejor desempeño que el azar.\n","\n","- ¿Cuál es el mejor clasificador entrenado?\n","\n","Si consideramos el recall como métrica de comparación, el mejor modelo es LightGBM\n","\n","- ¿Por qué el mejor clasificador es mejor que los otros?\n","\n","Dado que estamos buscando maximizar el recall, para que sean menor la cantidad de errores cuando predecimos clientes de riesgo.\n","\n","- Respecto al tiempo de entrenamiento, con cual cree que sería mejor experimentar (piense en el tiempo que le tomaría pasar el modelo por una grilla de optimización de hiperparámetros).\n","\n","En general los modelos de SVM o KNN, pueden tardar mucho en probar las distintas combinaciones de hiperparámetros, y a primera instancia no dieron muy buenos resultados. Por otro lado algoritmos basado en arboles como LightGBM, XGBoost o RandomForest, son mucho más eficientes en tiempo y permiten optimizar de mejor manera que el resto de modelos. Por lo que recomendaría probar experimentar con los modelos basado en arboles\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","### 5. Optimización del Modelo [1.5 puntos]\n","\n","\n","_En esta sección deben mejorar del modelo de clasificación al variar los algoritmos/hiperparámetros que están ocupando._\n","\n","- Instanciar dos nuevas `Pipeline`, similares a la anterior, pero ahora enfocada en buscar el mejor modelo. Para esto, la pipelines debe utilizar el primer y segundo mejor modelo encontrado en el paso anterior.\n","- Usar **`Optuna`** para tunear hiperparámetros\n","- **Importante**: Recuerden setear la búsqueda para optimizar la métrica seleccionada en los puntos anteriores.\n","\n","Algunas ideas para mejorar el rendimiento de sus modelos:\n","\n","- Agregar técnicas de seleccion de atributos/características. El parámetro de cuántas características se seleccionan debe ser parametrizable y configurado por el optimizador de hiperparámetros.\n","- Variar el imputador de datos en caso de usarlo.\n","\n","#### Bonus\n","\n","1. **Visualización con Optuna** [0.2 extras]: Explore la documentación de visualización de Optuna en el siguiente [link](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html) y realice un análisis sobre el proceso de optimización de hiperparámetros realizado.\n","2. **Imabalanced learn** [0.3 extras]: Al ser el problema desbalanceado, pueden probar técnicas para balancear automáticamente el dataset previo a ejecutar el modelo. Para esto, puede probar con los mecanismos implementados en la librería [Imbalanced learn](https://imbalanced-learn.org/). \n","3. **Probar pycaret (AutoML)** [0.3 extras].\n","\n","Algunas notas interesantes sobre este proceso:\n","\n","- No se les pide rendimientos cercanos al 100% de la métrica para concretar exitosamente el proyecto. Por otra parte, celebren cada progreso que obtengan.\n","- **Hacer grillas computables**: Si la grilla se va a demorar 1/3 la edad del universo en explorarse completamente, entonces achíquenla a algo que sepan que va a terminar.\n","- Aprovechen el procesamiento paralelo (con `njobs`) para acelerar la búsqueda. Sin embargo, si tienen problemas con la memoria RAM, reduzca la cantidad de jobs a algo que su computador/interprete web pueda procesar.\n","\n","**Al final de este proceso, seleccione el mejor modelo encontrado, prediga el conjunto de prueba y reporte sus resultados.**"]},{"cell_type":"code","execution_count":229,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----\n","Best Objective Value: 0.9409402093969023\n","Best Parameters: {'boosting_type': 'dart', 'num_leaves': 70, 'learning_rate': 0.04714571442216399, 'subsample_for_bin': 260653, 'min_child_samples': 72, 'reg_alpha': 0.0005736139035219997, 'reg_lambda': 5.0619665156456415e-05, 'colsample_bytree': 0.7820391704459884, 'max_depth': 3, 'bagging_fraction': 0.634526851836063, 'feature_fraction': 0.7274717219767692, 'bagging_freq': 6, 'min_child_weight': 7.137085265609137, 'scale_pos_weight': 9.532912998669016, 'subsample': 0.9748670451636554, 'min_split_gain': 0.7727704521600126}\n","Best Trial Number: 31\n","[LightGBM] [Warning] feature_fraction is set=0.7274717219767692, colsample_bytree=0.7820391704459884 will be ignored. Current value: feature_fraction=0.7274717219767692\n","[LightGBM] [Warning] bagging_fraction is set=0.634526851836063, subsample=0.9748670451636554 will be ignored. Current value: bagging_fraction=0.634526851836063\n","[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n","[LightGBM] [Warning] feature_fraction is set=0.7274717219767692, colsample_bytree=0.7820391704459884 will be ignored. Current value: feature_fraction=0.7274717219767692\n","[LightGBM] [Warning] bagging_fraction is set=0.634526851836063, subsample=0.9748670451636554 will be ignored. Current value: bagging_fraction=0.634526851836063\n","[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n","[LightGBM] [Info] Number of positive: 2537, number of negative: 6213\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002173 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 2372\n","[LightGBM] [Info] Number of data points in the train set: 8750, number of used features: 43\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.289943 -> initscore=-0.895662\n","[LightGBM] [Info] Start training from score -0.895662\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] feature_fraction is set=0.7274717219767692, colsample_bytree=0.7820391704459884 will be ignored. Current value: feature_fraction=0.7274717219767692\n","[LightGBM] [Warning] bagging_fraction is set=0.634526851836063, subsample=0.9748670451636554 will be ignored. Current value: bagging_fraction=0.634526851836063\n","[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n","----\n","Result Recall Test: 0.9964093357271095\n"]}],"source":["from sklearn.metrics import recall_score\n","import optuna\n","import warnings\n","# ...\n","\n","\n","# Explicado más arriba\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('cat', categorical_transformer, categorical_cols),\n","        ('num', numeric_transformer, numeric_cols)\n","   #     ('y',   'passthrough'  ,to_pred),\n","        \n","    ],\n","    sparse_threshold=0,\n","    remainder='passthrough'\n",")\n","\n","\n","\n","def objective_lightgbm(trial): \n","    # Definir espacio de búsqueda de hiperparámetros\n","    # Espacio de búsqueda de hiperparámetros para LightGBM\n","    param_space_lightgbm = {\n","        'boosting_type': trial.suggest_categorical('boosting_type', ['gbdt', 'dart']),\n","        'num_leaves': trial.suggest_int('num_leaves', 8, 150),\n","        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n","        'subsample_for_bin': trial.suggest_int('subsample_for_bin', 20000, 300000),\n","        'min_child_samples': trial.suggest_int('min_child_samples', 1, 200),\n","        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 5.0, log=True),\n","        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 5.0, log=True),\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n","        'max_depth': trial.suggest_int('max_depth', 3, 15),\n","        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n","        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n","        'bagging_freq': trial.suggest_int('bagging_freq', 0, 10),\n","        'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 10.0),\n","        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 10.0),\n","        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n","        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n","        'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 1.0),\n","    }\n","\n","    param_space = param_space_lightgbm\n","\n","    # Crear un pipeline con un imputador, selector de características y clasificador\n","    pipeline = Pipeline([\n","        ('preprocessor', preprocessor),\n","        ('knn_imputer', knn_imputer),\n","      #  ('selector', SelectKBest(f_classif, k=trial.suggest_int('k_best_features', 5, 15))),\n","        ('classifier', LGBMClassifier(**param_space))  # Puedes cambiar el clasificador aquí\n","    ])\n","\n","    # Ajustar el modelo al conjunto de entrenamiento\n","    pipeline.fit(X_train, y_train)\n","\n","    # Calcular el recall en el conjunto de validación\n","    \n","    y_pred_valid = pipeline.predict(X_val)\n","    recall = recall_score(y_val, y_pred_valid)\n","    f1_score_a = f1_score(y_val, y_pred_valid)\n","    print(\"----\")\n","    print(recall)\n","    print(f1_score_a)\n","    print(\"----\")\n","\n","    # utilizamos  \"pareto optimality\"para que el modelo no deje de lado metricas como f1-score que mejora al modelo en general, pero dandole muy poco peso a f1-score\n","    # Aunque en la instrucción dice que se optimize solo por una métrica, considere importante tomar en cuenta casos donde se mejora el recall, por un 1% a cambio de un\n","    # rendimiento en otras métricas que disminuya un 15% ( por ejemplo )\n","    alpha = 0.9\n","    metrica_compuesta = recall * alpha + f1_score_a * (1-alpha) \n","    return metrica_compuesta\n","\n","from IPython.utils import io\n","\n","\n","# Creamos la siguiente funcion para que no salga tanto output\n","with io.capture_output() as captured:\n","    # Crear el estudio de Optuna\n","    study = optuna.create_study(direction='maximize')  # Ahora maximizamos el recall\n","    study.optimize(objective_lightgbm, n_trials=100)  # Ajusta el número de trials según sea necesario\n","\n","print(\"----\")\n","# After optimization, get the best trial\n","best_trial = study.best_trial\n","\n","# Get the value of the objective function for the best trial\n","best_value = best_trial.value\n","\n","# Get other information about the best trial\n","best_params = best_trial.params\n","best_trial_number = best_trial.number\n","best_trial_user_attrs = best_trial.user_attrs\n","\n","# Print or use the results\n","print(\"Best Objective Value:\", best_value)\n","print(\"Best Parameters:\", best_params)\n","print(\"Best Trial Number:\", best_trial_number)\n","\n","\n","# Ahora tenemos que probar el modelo con el conjunto de testeo\n","# Crear y entrenar un nuevo modelo con los mejores parámetros\n","\n","pipeline = Pipeline([\n","        ('preprocessor', preprocessor),\n","        ('knn_imputer', knn_imputer),\n","      #  ('selector', SelectKBest(f_classif, k=trial.suggest_int('k_best_features', 5, 15))),\n","        ('classifier', LGBMClassifier(**best_params))  # Puedes cambiar el clasificador aquí\n","    ])\n","\n","    # Ajustar el modelo al conjunto de entrenamiento\n","pipeline.fit(X_train, y_train)\n","\n","    # Calcular el recall en el conjunto de validación\n","y_pred_test = pipeline.predict(X_test)\n","recall = recall_score(y_test, y_pred_test)\n","\n","print(\"----\")\n","print(\"Result Recall Test:\",recall )\n"]},{"cell_type":"code","execution_count":196,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Objective Value: 0.9960629921259843\n","Best Parameters: {'boosting_type': 'dart', 'num_leaves': 94, 'learning_rate': 0.04432455524352094, 'subsample_for_bin': 249092, 'min_child_samples': 94, 'reg_alpha': 0.0558645683360226, 'reg_lambda': 8.984561676604895e-09, 'colsample_bytree': 0.6127642462945145, 'max_depth': 3, 'bagging_fraction': 0.8994772650035048, 'feature_fraction': 0.7046230440729889, 'bagging_freq': 8, 'min_child_weight': 7.196447684243474, 'scale_pos_weight': 9.990181258818277, 'subsample': 0.6214750777904652, 'min_split_gain': 0.8585952341648521}\n","Best Trial Number: 92\n"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":230,"metadata":{},"outputs":[],"source":["####\n","# Abajo interpterabilidad"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### 6. Interpretabilidad [1.0 puntos]\n","\n","_En esta sección, se espera que los estudiantes demuestren su capacidad para explicar cómo sus modelos toman decisiones utilizando los datos. Dentro del análisis de interpretabilidad propuesto para el modelo, deberán ser capaces de:_\n","\n","- Proponer un método para analizar la interpretabilidad del modelo. Es crucial que puedan justificar por qué el método propuesto es el más adecuado y explicar los alcances que podría tener en su aplicación.\n","- Identificar las características más relevantes del modelo. ¿La distribución de importancia es coherente y equitativa entre todas las variables?\n","- Analizar 10 observaciones aleatorias utilizando un método específico para verificar la coherencia de las interacciones entre las características.\n","- Explorar cómo se relacionan las variables utilizando algún descriptivo de interpretabilidad.\n","- ¿Existen variables irrelevantes en el problema modelado?, ¿Cuales son?.\n","\n","Es fundamental que los estudiantes sean capaces de determinar si su modelo toma decisiones coherentes y evaluar el impacto que podría tener la aplicación de un modelo con esas variables en una población. ¿Es posible que el modelo sea perjudicial o que las estimaciones se basen en decisiones sesgadas?\n","\n","En resumen, esta sección busca que los estudiantes apliquen un enfoque crítico para evaluar la interpretabilidad de su modelo, identificar posibles sesgos y analizar las implicaciones de sus decisiones en la población objetivo.\n"]}],"metadata":{"deepnote":{},"deepnote_execution_queue":[],"deepnote_notebook_id":"4a4ad4306407402db0c258f95d5a9abe","kernelspec":{"display_name":"competencia","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
